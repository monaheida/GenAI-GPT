{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Build a GPT from scratch\n",
        "\n",
        "GPT is based on the Transformer architecture and is a decoder-only Transformer, meaning there is no encoder stack in the model. Like other Transformer models, GPT uses self-attention mechanisms to process input data in parallel, significantly improving the efficiency and effectiveness of training large language models.\n",
        "\n"
      ],
      "metadata": {
        "id": "vdQsIUfYkQer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the dataset!"
      ],
      "metadata": {
        "id": "XFqIXR3FEFeC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHOPuJZhDIz2",
        "outputId": "3d2c7d5c-984a-4f01-88e9-e7716fba29dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-25 10:03:29--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-03-25 10:03:29 (27.6 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "4DqGIbfiEPy4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-ztstzLESoH",
        "outputId": "53cc04ea-9736-4c79-a47d-45f425d63ff0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"Total unique characters:\", vocab_size, end=\"\\n\\n\")\n",
        "print(chars)\n",
        "print(''.join(chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTmaDABsEWoq",
        "outputId": "feaa1481-1d77-45ea-a30b-9b65849035e8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique characters: 65\n",
            "\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is only one number there \"3\""
      ],
      "metadata": {
        "id": "aE2zgWNJFJh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text.count(\"3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6jMHlHSFANG",
        "outputId": "69f7bb1d-7c38-4ea0-c3b5-4cd50c01b43c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_number = {ch:i for i,ch in enumerate(chars)}\n",
        "number_to_char = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "# encoder: take a string, output a list of integers\n",
        "encode = lambda s: [char_to_number[c] for c in s]\n",
        "\n",
        "# decoder: take a list of integers, output a string\n",
        "decode = lambda l: ''.join([number_to_char[i] for i in l])\n",
        "\n",
        "encoded_tokens = encode(\"This is GPT.\")\n",
        "print(encoded_tokens)\n",
        "\n",
        "decoded_tokens = decode(encoded_tokens)\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCzPCA1KFQEt",
        "outputId": "24c076b9-8ea6-42f7-f760-fadfed343c0a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[32, 46, 47, 57, 1, 47, 57, 1, 19, 28, 32, 8]\n",
            "This is GPT.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "90% for the training data\n",
        "\n",
        "10% for the validation data"
      ],
      "metadata": {
        "id": "GG87J2dfF1l1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)"
      ],
      "metadata": {
        "id": "d7akq_rzFpFM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dtype long to avoid numerical overflow\n",
        "\n",
        "print(data.shape, data.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nVT3ELKGLdx",
        "outputId": "0d66eb26-488b-478b-f7ea-29d56886c207"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "1BKha2QAGwP_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode(train_data[:100].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AYqNmgZdG9X_",
        "outputId": "76452bcf-56ab-4eb6-86cd-5961bd15eb75"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_window = 8\n",
        "sample_data = train_data[:context_window+1]\n",
        "print(sample_data, decode(sample_data.tolist()), sep=\" = \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yo6Epy8rHKSu",
        "outputId": "bb15ccfb-9e3e-41e3-9973-d8f37c31605c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58]) = First Cit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:context_window] # except the `+1`th character\n",
        "y = train_data[1:context_window+1]\n",
        "for t in range(context_window):\n",
        "    context = x[:t+1].tolist()\n",
        "    target = y[t]\n",
        "    print(f\"When input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dsjuc656Ols6",
        "outputId": "3952eda9-13ab-44dd-bc50-07c33b03d84a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When input is [18] the target: 47\n",
            "When input is [18, 47] the target: 56\n",
            "When input is [18, 47, 56] the target: 57\n",
            "When input is [18, 47, 56, 57] the target: 58\n",
            "When input is [18, 47, 56, 57, 58] the target: 1\n",
            "When input is [18, 47, 56, 57, 58, 1] the target: 15\n",
            "When input is [18, 47, 56, 57, 58, 1, 15] the target: 47\n",
            "When input is [18, 47, 56, 57, 58, 1, 15, 47] the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(context_window):\n",
        "    context = x[:t+1].tolist()\n",
        "    target = y[t].tolist()\n",
        "    print(f\"{decode(context)} → {decode([target])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKyvKcxwPCfU",
        "outputId": "3f7b8863-88a1-4243-8100-9435db7dd926"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F → i\n",
            "Fi → r\n",
            "Fir → s\n",
            "Firs → t\n",
            "First →  \n",
            "First  → C\n",
            "First C → i\n",
            "First Ci → t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NKZBZpRPQ2i",
        "outputId": "b9b3bfcf-fa2e-4ed9-c6d6-fc78a6fcea43"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1003854])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 4       # batch_size\n",
        "context_window = 8  # block_size\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - context_window, (n_samples,))\n",
        "    x = torch.stack([data[i:i+context_window] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+context_window+1] for i in ix])\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "KqoijClpQAqO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch('train')\n",
        "print('Inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('\\nTargets:')\n",
        "print(yb.shape)\n",
        "print(yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksdlhevdRZTg",
        "outputId": "c372463c-6b37-40f1-8d5a-3ce2e58a308d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[52, 42,  8,  0,  0, 23, 21, 26],\n",
            "        [45, 53, 42, 57,  0, 23, 43, 43],\n",
            "        [52,  1, 61, 39, 57,  1, 51, 53],\n",
            "        [39, 49, 12,  1, 27,  1, 58, 56]])\n",
            "\n",
            "Targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[42,  8,  0,  0, 23, 21, 26, 19],\n",
            "        [53, 42, 57,  0, 23, 43, 43, 54],\n",
            "        [ 1, 61, 39, 57,  1, 51, 53, 56],\n",
            "        [49, 12,  1, 27,  1, 58, 56, 39]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Inputs:')\n",
        "for decoded_xb in map(lambda t: decode(t.tolist()), xb):\n",
        "    print(\"[\", decoded_xb.replace(\"\\n\",\"\\\\n\"), \"]\", sep='')\n",
        "\n",
        "print('\\nTargets:')\n",
        "for decoded_yb in map(lambda t: decode(t.tolist()), yb):\n",
        "    print(\"[\", decoded_yb.replace(\"\\n\",\"\\\\n\"), \"]\", sep='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8Z-LS_xSd2L",
        "outputId": "188bdbe1-707a-49e3-bb40-037755d864be"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            "[nd.\\n\\nKIN]\n",
            "[gods\\nKee]\n",
            "[n was mo]\n",
            "[ak? O tr]\n",
            "\n",
            "Targets:\n",
            "[d.\\n\\nKING]\n",
            "[ods\\nKeep]\n",
            "[ was mor]\n",
            "[k? O tra]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigram LM\n",
        "\n",
        "\n",
        "\n",
        "*   The model only takes care of the previous token\n",
        "*   Based on that, it will select the row (in the manual case) and will pick out the next most probable character.\n",
        "\n"
      ],
      "metadata": {
        "id": "s4VzXU_RbjSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzc7KYYUaov4",
        "outputId": "b7bb2547-4e54-44cc-d3d8-2a77e3d651eb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7812ccb9da10>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLM(nn.Module):\n",
        "    \"\"\"\n",
        "    In the very next cell --> implement full BigramLM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # The simple lookup table...\n",
        "        self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        '''\n",
        "        Just take the index of the \"current\" character\n",
        "        and then use that to get the probability dist\n",
        "        for the next one using the `embedding_table`.\n",
        "        '''\n",
        "\n",
        "        logits = self.embedding_table(idx)\n",
        "\n",
        "        return logits # Form of (B, T, C)"
      ],
      "metadata": {
        "id": "GzNiuTpDcMNv"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLM(vocab_size)\n",
        "logits = model(xb, yb)\n",
        "logits.shape # B, T, C"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl2dlvRrcrAI",
        "outputId": "089718b8-054c-4488-cd0c-094ee5d42196"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 65])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   4: Number of samples\n",
        "*   8: Context window (each tokens)\n",
        "*   65: The vocab size (distributions of next token for each tokens in context window)"
      ],
      "metadata": {
        "id": "7Zlu_7ysdIEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  Input: You have a text corpus from which you're training your model.\n",
        "*  Tokenization: You tokenize the text into characters. Each character becomes a token.\n",
        "*   Character Distribution: For each character in the corpus, you analyze the distribution of the next character that follows it.\n",
        "*  Sampling: When generating new text, you start with an initial character and sample the next character based on the distribution you've learned for that character. Repeat this process iteratively to generate a sequence of characters.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mt3A_0Pxdlwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Builing Bigram model"
      ],
      "metadata": {
        "id": "SaujDJ58eINa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLM(nn.Module):\n",
        "    \"\"\"\n",
        "    This class takes the `vocab_size` as a single input because its being\n",
        "    the \"simplest\" model, we won't need anything else.\n",
        "\n",
        "    It will create `vocab_size` * `vocab_size` table and then we will be\n",
        "    able to access it.\n",
        "\n",
        "    The Forward function will take the `x` input and based on the shape\n",
        "    it will perform the forward pass on it. The nuances are explained in\n",
        "    the following markdown cells.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        '''\n",
        "        Just take the index of the \"current\" character\n",
        "        and then use that to get the probability dist\n",
        "        for the next one using the `embedding_table`.\n",
        "        '''\n",
        "\n",
        "        logits = self.embedding_table(idx)\n",
        "\n",
        "        if targets is None: # inferencing and not training\n",
        "            loss=None\n",
        "        else:               # training\n",
        "            # Refer: Change [1]\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "\n",
        "            # Refer: Change [1]\n",
        "            targets = targets.view(B*T)\n",
        "\n",
        "            # For the given logits and *correct* targets, pick the pre-\n",
        "            # dicted logits for the given target to calculate the\n",
        "            # negative log likelihood loss.\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        '''\n",
        "        Take the index of the token and guess the next\n",
        "        token based on the embeddings!\n",
        "        '''\n",
        "        for _ in range(max_new_tokens):\n",
        "            # call the `forward` method\n",
        "            logits, loss = self(idx) # It's possible bc we have inherited `nn.Module`\n",
        "\n",
        "            # Take the very last token (8th in the context window)\n",
        "            # and use its distribution to get the next token\n",
        "            logits = logits[:, -1, :] ## refer: Change [2]\n",
        "\n",
        "            # Convert the logits into the probabilities\n",
        "            probs = F.softmax(logits, dim=-1) ## dim=-1: along the last dimension ~ here `1`\n",
        "\n",
        "            # Take the next idx!\n",
        "            next_idx = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Here we will ALWAYS append, there is NO shrinking!\n",
        "            idx = torch.cat((idx, next_idx), dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "wjRFAR9Ncv5p"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generation of a un-trained random model\n",
        "model = BigramLM(vocab_size)\n",
        "logits, loss = model(xb, yb)\n",
        "output = decode(\n",
        "    model.generate(\n",
        "        idx = torch.zeros((1, 1), dtype=torch.long),\n",
        "        max_new_tokens=100)[0].tolist()\n",
        ")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObunJtNChxV7",
        "outputId": "9cc6e383-aa98-464e-e4d1-f0f88e10cda1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "hbH\n",
            "\n",
            ":CLP.A!fq'3ggt!O!T?X!!SA?W&TrpvYybSE3w&S BXUhmiKYyTmWMPhhmnHKj!!btgnwNNULuEzRuYyiWEQxPX!$3C'MBj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization"
      ],
      "metadata": {
        "id": "97SVuMC4h-Tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLM(vocab_size)"
      ],
      "metadata": {
        "id": "mHjNcomnh2Qw"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "3_QzH-jpiGMS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We only have a simple, single layer... the parameters of that layer, which is 65 will be returned."
      ],
      "metadata": {
        "id": "-PwPhLOxiUDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param = model.parameters()\n",
        "for p in param:\n",
        "    print(len(p))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0qZDoWTiJZ_",
        "outputId": "b47ed46f-f065-47eb-8f86-8cf20dc25128"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "stknCPQ3iq4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 32 # batch size\n",
        "\n",
        "for steps in range(20_000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzPzXfW6ibwO",
        "outputId": "8c4f3abd-9548-4a26-bbd0-4d73081c3696"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.420738458633423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = decode(\n",
        "    model.generate(\n",
        "        idx = torch.zeros((1, 1), dtype=torch.long),\n",
        "        max_new_tokens=500)[0].tolist()\n",
        ")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJJRqfM2i1fq",
        "outputId": "575d93a2-0279-4578-85b8-1ac03714dc7c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Toul sil prangir sis.\n",
            "\n",
            "Wh I whise brthit RD:\n",
            "Gom.\n",
            "I INomere y ghesen cond\n",
            "YCouchin, w?\n",
            "\n",
            "Te counere ne ung;\n",
            "GE n;\n",
            "II t.\n",
            "\n",
            "He verve o was.\n",
            "\n",
            "\n",
            "Hes aift NTHEDIO:\n",
            "Sl:\n",
            "Whinke ngt?\n",
            "MAms NGO shemo too mo anthatinthakes f utous as Agonteopr botherore thind spat PTEShiat ureraierio pr son me LO:\n",
            "Wats me S:\n",
            "To tllingewe lley ayom\n",
            "\n",
            "Mo;\n",
            "Latanssuromas:\n",
            "\n",
            "Y:\n",
            "PE:\n",
            "Therucover, min ld te o e, un rd o s hthecals,\n",
            "\n",
            "WI thank m:\n",
            "\n",
            "\n",
            "NTharu t irlendoucin,\n",
            "Y: Tisteriomad t.\n",
            "Yor f.\n",
            "\n",
            "S:\n",
            "G, g w,\n",
            "\n",
            "Hee:\n",
            "\n",
            "\n",
            "NGLI,\n",
            "JUTeden t t IVo sl\n"
          ]
        }
      ]
    }
  ]
}